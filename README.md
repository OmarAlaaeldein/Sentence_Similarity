# Sentence_Similarity

This project is an exploration into using word embeddings to analyze sentence similarity. The novelty of this approach lies in not needing a full transformer encoder block, offering a lightweight alternative for tasks that require sentence similarity analysis.

## Approach

The methodology involves creating sentence vectors from word embeddings, then calculating the cosine similarity between these sentence vectors to gauge the similarity between different sentences. The word embeddings are obtained from a FastText model trained on a diverse corpus.

## Progress

The current state of the project includes:

- A locally trained FastText model on a diverse set of text corpora, including data from Reddit, Twitter, and other internet sources.
- A function to create sentence vectors from the word embeddings generated by the FastText model.
- A function to calculate cosine similarity between sentence vectors.

## To-Do List

- [ ] Train the FastText model on a more extensive corpus to better generalize the embeddings.
- [ ] Experiment with varying dimensions for the word embeddings.
- [X] Modify the sentence_vector function to normalize vectors at each step to prevent the vanishing gradient problem.
- [X] Test the sentence similarity function with a variety of sentences to evaluate robustness.
- [ ] Assess the model performance using standard NLP evaluation metrics.
- [ ] Implement the model with Word2Vec and GloVe and compare the performance.
- [ ] Investigate potential for parallelizing operations to enhance computation speed and scalability.

## Note

The pretrained embedding model is trained on a diverse set of text corpora scraped from various internet sources. Therefore, it should generalize well to a wide variety of text types. While this project is still in the experimental phase, the simplicity and lightweight nature of this approach make it a compelling avenue to explore for tasks requiring sentence similarity computation.

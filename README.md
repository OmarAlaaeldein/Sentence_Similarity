# Sentence_Similarity

This project explores the idea of using word embeddings to analyze sentence similarity. It implements a novel approach that doesn't require a full transformer encoder block, making it a lightweight alternative for simple sentence similarity tasks.

## Approach

The technique employed involves creating sentence vectors from word embeddings by aggregating them using specific mathematical operations. Then, the cosine similarity between these sentence vectors is calculated to determine the similarity between the sentences.

## Progress

The current state of the project includes:

- A trained FastText model on a locally aggregated text corpus.
- A function to create sentence vectors from the word embeddings generated by the FastText model.
- A function to calculate cosine similarity between sentence vectors.

## To-Do List

- [ ] Train the FastText model on a larger corpus for better embeddings.
- [ ] Experiment with different dimensions for the word embeddings.
- [X] Modify the sentence_vector function to normalize vectors at each step to prevent the vanishing gradient problem.
- [X] Test the sentence similarity function with a wide variety of sentences to check robustness.
- [ ] Evaluate the performance of the model using standard NLP evaluation metrics.
- [ ] Implement the model with Word2Vec and GloVe, and compare the performance.
- [ ] Explore the possibility of parallelizing operations for faster computation and scalability.

## Note

The pretrained embedding model is trained on an aggregation of local text corpora, and it may not generalize well to very diverse or specialized types of text. This project is currently in the experimental phase, and for the objective of comparing sentence similarity, it may not perform as optimally as more complex methods such as transformer-based models. However, the simplicity and lightweight nature of this approach make it worth exploring for certain use-cases.
